{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PPvHCE_jaFYs"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda import random\n",
        "device = torch.device(\"cuda0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "random_seed = 1\n",
        "learning_rate = 0.1\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "dropout_prob = 0.5\n",
        "\n",
        "num_features = 784\n",
        "num_hidden_1 = 128\n",
        "num_hidden_2 = 256\n",
        "num_classes = 10\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"data\",train=True,transform=transforms.ToTensor(),download=True)\n",
        "test_dataset = datasets.MNIST(root=\"data\",train=False,transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "for images, labels in train_loader:\n",
        "  print(\"Image batch dimensions:\",images.shape)\n",
        "  print(\"Image label dimensions:\",labels.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY3p2BSkcyHk",
        "outputId": "998943a8-ff42-4f9c-e51d-4a5231e57d75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 495kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.66MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.71MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(torch.nn.Module):\n",
        "  def __init__(self,num_features,num_classes):\n",
        "    super(MLP,self).__init__()\n",
        "    self.linear_1 = torch.nn.Linear(num_features,num_hidden_1)\n",
        "    self.linear_1.weight.detach().normal_(0.0,0.1)\n",
        "    self.linear_1.bias.detach().zero_()\n",
        "\n",
        "    self.linear_2 = torch.nn.Linear(num_hidden_1,num_hidden_2)\n",
        "    self.linear_2.weight.detach().normal_(0.0,0.1)\n",
        "    self.linear_2.bias.detach().zero_()\n",
        "\n",
        "    self.linear_out = torch.nn.Linear(num_hidden_2,num_classes)\n",
        "    self.linear_out.weight.detach().normal_(0.0,0.1)\n",
        "    self.linear_out.bias.detach().zero_()\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = self.linear_1(x)\n",
        "    out = F.relu(out)\n",
        "    out = F.dropout(out, p=dropout_prob, training=self.training)\n",
        "    out = self.linear_2(out)\n",
        "    out = F.relu(out)\n",
        "    out = F.dropout(out, p=dropout_prob, training=self.training)\n",
        "    logits = self.linear_out(out)\n",
        "    probas = F.softmax(logits,dim=1)\n",
        "    return logits,probas\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "model = MLP(num_features=num_features, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "aEBt-HI_e5o1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(net,data_loader):\n",
        "  net.eval()\n",
        "  correct_pred, num_examples = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for features, targets in data_loader:\n",
        "      features = features.view(-1,28*28).to(device)\n",
        "      targets = targets.to(device)\n",
        "      logits, probas = net(features)\n",
        "      _,predicted_labels = torch.max(probas,1)\n",
        "      num_examples += targets.size(0)\n",
        "      correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "    features = features.view(-1,28*28).to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    logits, probas = model(features)\n",
        "    cost = F.cross_entropy(logits,targets)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    cost.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if not batch_idx % 50:\n",
        "      print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f'\n",
        "                   %(epoch+1, num_epochs, batch_idx,\n",
        "                     len(train_loader), cost))\n",
        "  print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
        "          epoch+1, num_epochs,\n",
        "          compute_accuracy(model, train_loader)))\n",
        "\n",
        "  print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIFWqc_3jF2f",
        "outputId": "12c840b5-8e78-49f9-b712-30e051ed7bcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/010 | Batch 000/938 | Cost: 0.5800\n",
            "Epoch: 001/010 | Batch 050/938 | Cost: 0.4650\n",
            "Epoch: 001/010 | Batch 100/938 | Cost: 0.4873\n",
            "Epoch: 001/010 | Batch 150/938 | Cost: 0.5862\n",
            "Epoch: 001/010 | Batch 200/938 | Cost: 0.2436\n",
            "Epoch: 001/010 | Batch 250/938 | Cost: 0.2921\n",
            "Epoch: 001/010 | Batch 300/938 | Cost: 0.3975\n",
            "Epoch: 001/010 | Batch 350/938 | Cost: 0.4826\n",
            "Epoch: 001/010 | Batch 400/938 | Cost: 0.3383\n",
            "Epoch: 001/010 | Batch 450/938 | Cost: 0.4192\n",
            "Epoch: 001/010 | Batch 500/938 | Cost: 0.5176\n",
            "Epoch: 001/010 | Batch 550/938 | Cost: 0.2289\n",
            "Epoch: 001/010 | Batch 600/938 | Cost: 0.3065\n",
            "Epoch: 001/010 | Batch 650/938 | Cost: 0.2995\n",
            "Epoch: 001/010 | Batch 700/938 | Cost: 0.4396\n",
            "Epoch: 001/010 | Batch 750/938 | Cost: 0.2114\n",
            "Epoch: 001/010 | Batch 800/938 | Cost: 0.4689\n",
            "Epoch: 001/010 | Batch 850/938 | Cost: 0.1430\n",
            "Epoch: 001/010 | Batch 900/938 | Cost: 0.4208\n",
            "Epoch: 001/010 training accuracy: 94.62%\n",
            "Time elapsed: 0.30 min\n",
            "Epoch: 002/010 | Batch 000/938 | Cost: 0.5103\n",
            "Epoch: 002/010 | Batch 050/938 | Cost: 0.1765\n",
            "Epoch: 002/010 | Batch 100/938 | Cost: 0.4225\n",
            "Epoch: 002/010 | Batch 150/938 | Cost: 0.4059\n",
            "Epoch: 002/010 | Batch 200/938 | Cost: 0.3204\n",
            "Epoch: 002/010 | Batch 250/938 | Cost: 0.3162\n",
            "Epoch: 002/010 | Batch 300/938 | Cost: 0.5313\n",
            "Epoch: 002/010 | Batch 350/938 | Cost: 0.1879\n",
            "Epoch: 002/010 | Batch 400/938 | Cost: 0.3024\n",
            "Epoch: 002/010 | Batch 450/938 | Cost: 0.2778\n",
            "Epoch: 002/010 | Batch 500/938 | Cost: 0.3625\n",
            "Epoch: 002/010 | Batch 550/938 | Cost: 0.1290\n",
            "Epoch: 002/010 | Batch 600/938 | Cost: 0.2720\n",
            "Epoch: 002/010 | Batch 650/938 | Cost: 0.1823\n",
            "Epoch: 002/010 | Batch 700/938 | Cost: 0.5531\n",
            "Epoch: 002/010 | Batch 750/938 | Cost: 0.1025\n",
            "Epoch: 002/010 | Batch 800/938 | Cost: 0.4097\n",
            "Epoch: 002/010 | Batch 850/938 | Cost: 0.3028\n",
            "Epoch: 002/010 | Batch 900/938 | Cost: 0.3479\n",
            "Epoch: 002/010 training accuracy: 95.46%\n",
            "Time elapsed: 0.60 min\n",
            "Epoch: 003/010 | Batch 000/938 | Cost: 0.3949\n",
            "Epoch: 003/010 | Batch 050/938 | Cost: 0.2025\n",
            "Epoch: 003/010 | Batch 100/938 | Cost: 0.2628\n",
            "Epoch: 003/010 | Batch 150/938 | Cost: 0.3893\n",
            "Epoch: 003/010 | Batch 200/938 | Cost: 0.3907\n",
            "Epoch: 003/010 | Batch 250/938 | Cost: 0.2002\n",
            "Epoch: 003/010 | Batch 300/938 | Cost: 0.2450\n",
            "Epoch: 003/010 | Batch 350/938 | Cost: 0.2666\n",
            "Epoch: 003/010 | Batch 400/938 | Cost: 0.1957\n",
            "Epoch: 003/010 | Batch 450/938 | Cost: 0.3350\n",
            "Epoch: 003/010 | Batch 500/938 | Cost: 0.3125\n",
            "Epoch: 003/010 | Batch 550/938 | Cost: 0.6231\n",
            "Epoch: 003/010 | Batch 600/938 | Cost: 0.2001\n",
            "Epoch: 003/010 | Batch 650/938 | Cost: 0.2916\n",
            "Epoch: 003/010 | Batch 700/938 | Cost: 0.1340\n",
            "Epoch: 003/010 | Batch 750/938 | Cost: 0.1774\n",
            "Epoch: 003/010 | Batch 800/938 | Cost: 0.0986\n",
            "Epoch: 003/010 | Batch 850/938 | Cost: 0.4114\n",
            "Epoch: 003/010 | Batch 900/938 | Cost: 0.2651\n",
            "Epoch: 003/010 training accuracy: 96.14%\n",
            "Time elapsed: 0.90 min\n",
            "Epoch: 004/010 | Batch 000/938 | Cost: 0.3343\n",
            "Epoch: 004/010 | Batch 050/938 | Cost: 0.2546\n",
            "Epoch: 004/010 | Batch 100/938 | Cost: 0.1639\n",
            "Epoch: 004/010 | Batch 150/938 | Cost: 0.3385\n",
            "Epoch: 004/010 | Batch 200/938 | Cost: 0.3275\n",
            "Epoch: 004/010 | Batch 250/938 | Cost: 0.1963\n",
            "Epoch: 004/010 | Batch 300/938 | Cost: 0.1748\n",
            "Epoch: 004/010 | Batch 350/938 | Cost: 0.3260\n",
            "Epoch: 004/010 | Batch 400/938 | Cost: 0.3380\n",
            "Epoch: 004/010 | Batch 450/938 | Cost: 0.1720\n",
            "Epoch: 004/010 | Batch 500/938 | Cost: 0.2886\n",
            "Epoch: 004/010 | Batch 550/938 | Cost: 0.3074\n",
            "Epoch: 004/010 | Batch 600/938 | Cost: 0.1285\n",
            "Epoch: 004/010 | Batch 650/938 | Cost: 0.2044\n",
            "Epoch: 004/010 | Batch 700/938 | Cost: 0.2389\n",
            "Epoch: 004/010 | Batch 750/938 | Cost: 0.1701\n",
            "Epoch: 004/010 | Batch 800/938 | Cost: 0.2075\n",
            "Epoch: 004/010 | Batch 850/938 | Cost: 0.2572\n",
            "Epoch: 004/010 | Batch 900/938 | Cost: 0.1154\n",
            "Epoch: 004/010 training accuracy: 96.48%\n",
            "Time elapsed: 1.21 min\n",
            "Epoch: 005/010 | Batch 000/938 | Cost: 0.1414\n",
            "Epoch: 005/010 | Batch 050/938 | Cost: 0.3945\n",
            "Epoch: 005/010 | Batch 100/938 | Cost: 0.1425\n",
            "Epoch: 005/010 | Batch 150/938 | Cost: 0.1100\n",
            "Epoch: 005/010 | Batch 200/938 | Cost: 0.1663\n",
            "Epoch: 005/010 | Batch 250/938 | Cost: 0.4667\n",
            "Epoch: 005/010 | Batch 300/938 | Cost: 0.1526\n",
            "Epoch: 005/010 | Batch 350/938 | Cost: 0.2609\n",
            "Epoch: 005/010 | Batch 400/938 | Cost: 0.0558\n",
            "Epoch: 005/010 | Batch 450/938 | Cost: 0.1452\n",
            "Epoch: 005/010 | Batch 500/938 | Cost: 0.1520\n",
            "Epoch: 005/010 | Batch 550/938 | Cost: 0.3013\n",
            "Epoch: 005/010 | Batch 600/938 | Cost: 0.2248\n",
            "Epoch: 005/010 | Batch 650/938 | Cost: 0.1332\n",
            "Epoch: 005/010 | Batch 700/938 | Cost: 0.1681\n",
            "Epoch: 005/010 | Batch 750/938 | Cost: 0.2366\n",
            "Epoch: 005/010 | Batch 800/938 | Cost: 0.3064\n",
            "Epoch: 005/010 | Batch 850/938 | Cost: 0.2265\n",
            "Epoch: 005/010 | Batch 900/938 | Cost: 0.1174\n",
            "Epoch: 005/010 training accuracy: 96.86%\n",
            "Time elapsed: 1.50 min\n",
            "Epoch: 006/010 | Batch 000/938 | Cost: 0.4843\n",
            "Epoch: 006/010 | Batch 050/938 | Cost: 0.1643\n",
            "Epoch: 006/010 | Batch 100/938 | Cost: 0.0990\n",
            "Epoch: 006/010 | Batch 150/938 | Cost: 0.0699\n",
            "Epoch: 006/010 | Batch 200/938 | Cost: 0.0837\n",
            "Epoch: 006/010 | Batch 250/938 | Cost: 0.2073\n",
            "Epoch: 006/010 | Batch 300/938 | Cost: 0.2714\n",
            "Epoch: 006/010 | Batch 350/938 | Cost: 0.3119\n",
            "Epoch: 006/010 | Batch 400/938 | Cost: 0.2037\n",
            "Epoch: 006/010 | Batch 450/938 | Cost: 0.1016\n",
            "Epoch: 006/010 | Batch 500/938 | Cost: 0.4769\n",
            "Epoch: 006/010 | Batch 550/938 | Cost: 0.0988\n",
            "Epoch: 006/010 | Batch 600/938 | Cost: 0.4160\n",
            "Epoch: 006/010 | Batch 650/938 | Cost: 0.1457\n",
            "Epoch: 006/010 | Batch 700/938 | Cost: 0.2744\n",
            "Epoch: 006/010 | Batch 750/938 | Cost: 0.1452\n",
            "Epoch: 006/010 | Batch 800/938 | Cost: 0.1255\n",
            "Epoch: 006/010 | Batch 850/938 | Cost: 0.1186\n",
            "Epoch: 006/010 | Batch 900/938 | Cost: 0.1636\n",
            "Epoch: 006/010 training accuracy: 97.12%\n",
            "Time elapsed: 1.81 min\n",
            "Epoch: 007/010 | Batch 000/938 | Cost: 0.0279\n",
            "Epoch: 007/010 | Batch 050/938 | Cost: 0.3603\n",
            "Epoch: 007/010 | Batch 100/938 | Cost: 0.0925\n",
            "Epoch: 007/010 | Batch 150/938 | Cost: 0.0617\n",
            "Epoch: 007/010 | Batch 200/938 | Cost: 0.1881\n",
            "Epoch: 007/010 | Batch 250/938 | Cost: 0.1763\n",
            "Epoch: 007/010 | Batch 300/938 | Cost: 0.1270\n",
            "Epoch: 007/010 | Batch 350/938 | Cost: 0.3087\n",
            "Epoch: 007/010 | Batch 400/938 | Cost: 0.1014\n",
            "Epoch: 007/010 | Batch 450/938 | Cost: 0.1739\n",
            "Epoch: 007/010 | Batch 500/938 | Cost: 0.2505\n",
            "Epoch: 007/010 | Batch 550/938 | Cost: 0.0913\n",
            "Epoch: 007/010 | Batch 600/938 | Cost: 0.1410\n",
            "Epoch: 007/010 | Batch 650/938 | Cost: 0.1367\n",
            "Epoch: 007/010 | Batch 700/938 | Cost: 0.1155\n",
            "Epoch: 007/010 | Batch 750/938 | Cost: 0.1506\n",
            "Epoch: 007/010 | Batch 800/938 | Cost: 0.3277\n",
            "Epoch: 007/010 | Batch 850/938 | Cost: 0.3572\n",
            "Epoch: 007/010 | Batch 900/938 | Cost: 0.2945\n",
            "Epoch: 007/010 training accuracy: 97.35%\n",
            "Time elapsed: 2.11 min\n",
            "Epoch: 008/010 | Batch 000/938 | Cost: 0.1474\n",
            "Epoch: 008/010 | Batch 050/938 | Cost: 0.1391\n",
            "Epoch: 008/010 | Batch 100/938 | Cost: 0.2983\n",
            "Epoch: 008/010 | Batch 150/938 | Cost: 0.0426\n",
            "Epoch: 008/010 | Batch 200/938 | Cost: 0.1324\n",
            "Epoch: 008/010 | Batch 250/938 | Cost: 0.1342\n",
            "Epoch: 008/010 | Batch 300/938 | Cost: 0.1674\n",
            "Epoch: 008/010 | Batch 350/938 | Cost: 0.1216\n",
            "Epoch: 008/010 | Batch 400/938 | Cost: 0.2264\n",
            "Epoch: 008/010 | Batch 450/938 | Cost: 0.2133\n",
            "Epoch: 008/010 | Batch 500/938 | Cost: 0.1762\n",
            "Epoch: 008/010 | Batch 550/938 | Cost: 0.0991\n",
            "Epoch: 008/010 | Batch 600/938 | Cost: 0.1713\n",
            "Epoch: 008/010 | Batch 650/938 | Cost: 0.1125\n",
            "Epoch: 008/010 | Batch 700/938 | Cost: 0.0817\n",
            "Epoch: 008/010 | Batch 750/938 | Cost: 0.2160\n",
            "Epoch: 008/010 | Batch 800/938 | Cost: 0.0995\n",
            "Epoch: 008/010 | Batch 850/938 | Cost: 0.2252\n",
            "Epoch: 008/010 | Batch 900/938 | Cost: 0.1870\n",
            "Epoch: 008/010 training accuracy: 97.50%\n",
            "Time elapsed: 2.42 min\n",
            "Epoch: 009/010 | Batch 000/938 | Cost: 0.1688\n",
            "Epoch: 009/010 | Batch 050/938 | Cost: 0.0632\n",
            "Epoch: 009/010 | Batch 100/938 | Cost: 0.0604\n",
            "Epoch: 009/010 | Batch 150/938 | Cost: 0.1494\n",
            "Epoch: 009/010 | Batch 200/938 | Cost: 0.4011\n",
            "Epoch: 009/010 | Batch 250/938 | Cost: 0.2123\n",
            "Epoch: 009/010 | Batch 300/938 | Cost: 0.2717\n",
            "Epoch: 009/010 | Batch 350/938 | Cost: 0.1933\n",
            "Epoch: 009/010 | Batch 400/938 | Cost: 0.2900\n",
            "Epoch: 009/010 | Batch 450/938 | Cost: 0.0433\n",
            "Epoch: 009/010 | Batch 500/938 | Cost: 0.2367\n",
            "Epoch: 009/010 | Batch 550/938 | Cost: 0.2523\n",
            "Epoch: 009/010 | Batch 600/938 | Cost: 0.1171\n",
            "Epoch: 009/010 | Batch 650/938 | Cost: 0.1663\n",
            "Epoch: 009/010 | Batch 700/938 | Cost: 0.1216\n",
            "Epoch: 009/010 | Batch 750/938 | Cost: 0.0836\n",
            "Epoch: 009/010 | Batch 800/938 | Cost: 0.1378\n",
            "Epoch: 009/010 | Batch 850/938 | Cost: 0.1060\n",
            "Epoch: 009/010 | Batch 900/938 | Cost: 0.0879\n",
            "Epoch: 009/010 training accuracy: 97.52%\n",
            "Time elapsed: 2.71 min\n",
            "Epoch: 010/010 | Batch 000/938 | Cost: 0.1971\n",
            "Epoch: 010/010 | Batch 050/938 | Cost: 0.1164\n",
            "Epoch: 010/010 | Batch 100/938 | Cost: 0.2593\n",
            "Epoch: 010/010 | Batch 150/938 | Cost: 0.1225\n",
            "Epoch: 010/010 | Batch 200/938 | Cost: 0.0598\n",
            "Epoch: 010/010 | Batch 250/938 | Cost: 0.1309\n",
            "Epoch: 010/010 | Batch 300/938 | Cost: 0.1719\n",
            "Epoch: 010/010 | Batch 350/938 | Cost: 0.1953\n",
            "Epoch: 010/010 | Batch 400/938 | Cost: 0.3677\n",
            "Epoch: 010/010 | Batch 450/938 | Cost: 0.1266\n",
            "Epoch: 010/010 | Batch 500/938 | Cost: 0.2490\n",
            "Epoch: 010/010 | Batch 550/938 | Cost: 0.3992\n",
            "Epoch: 010/010 | Batch 600/938 | Cost: 0.2622\n",
            "Epoch: 010/010 | Batch 650/938 | Cost: 0.1040\n",
            "Epoch: 010/010 | Batch 700/938 | Cost: 0.1530\n",
            "Epoch: 010/010 | Batch 750/938 | Cost: 0.1733\n",
            "Epoch: 010/010 | Batch 800/938 | Cost: 0.1917\n",
            "Epoch: 010/010 | Batch 850/938 | Cost: 0.0944\n",
            "Epoch: 010/010 | Batch 900/938 | Cost: 0.2313\n",
            "Epoch: 010/010 training accuracy: 97.77%\n",
            "Time elapsed: 3.00 min\n",
            "Total Training Time: 3.00 min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP_JeOdylrFP",
        "outputId": "6993dc80-3a2e-4685-c1c1-5474532f85c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 97.09%\n"
          ]
        }
      ]
    }
  ]
}